{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "designed-translation",
   "metadata": {},
   "source": [
    "# HW1 - Policy Iteration for Taxi Cab Problem\n",
    "\n",
    "We know that, a policy iteration algorithm can be subdivided into two components:\n",
    "\n",
    "policy evaluation and\n",
    "policy improvement.\n",
    "It starts with an arbitrary policy. And in each iteration, it\n",
    "\n",
    "first computes the policy values given the latest policy, based on the Bellman expectation equation; it then\n",
    "extracts an improved policy out of the resulting policy values, based on the Bellman optimality equation. It iteratively evaluates the policy and generates an improved version until the policy doesn't change any more.\n",
    "In this work, a policy iteration algorithm is developed to solve the Taxi Cab OpenAI Gym Problem\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "indie-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "from time import sleep\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-carolina",
   "metadata": {},
   "source": [
    "## 1.  The taxi game environment\n",
    "The aim of the taxi game is to make sure the taxi can get to the passenger, pick him up and bring him to the drop-off location in the fastest way possible.\n",
    "\n",
    "### Representations\n",
    "\n",
    "| $\\rightarrow$ WALL (Can't pass through, will remain in the same position if tries to move through wall)\n",
    "\n",
    "Yellow $\\rightarrow$ Taxi Current Location\n",
    "\n",
    "Blue $\\rightarrow$ Pick up Location\n",
    "\n",
    "Purple $\\rightarrow$ Drop-off Location\n",
    "\n",
    "Green $\\rightarrow$ Taxi turn green once passenger board\n",
    "\n",
    "Letters $\\rightarrow$ Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "explicit-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-prescription",
   "metadata": {},
   "source": [
    "Total Number of states in the environment = **500** (0 to 499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "warming-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n   #Total no. of states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-surgery",
   "metadata": {},
   "source": [
    "**Actions (6 in total)**\n",
    "\n",
    "0: move south  \n",
    "1: move north  \n",
    "2: move east  \n",
    "3: move west \n",
    "4: pickup passenger  \n",
    "5: dropoff passenger  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developmental-quality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n   #Total no. of actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historical-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.env.s = 122\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "virtual-tradition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-french",
   "metadata": {},
   "source": [
    "Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "The 4 elements returned are:\n",
    "\n",
    "- **Observation (object)**: the state the environment is in or an environment-specific object representing your observation of the environment.\n",
    "- **Reward (float)**: Reward achieved by the previous action.\n",
    "> - +20: Last step when we successfully pick up a passenger and drop them off at their desired location  \n",
    "> - -1: for each step in order for the agent to try and find the quickest solution possible  \n",
    "> - -10: every time you incorrectly pick up or drop off a passenger  \n",
    "-**Done (boolean)**: whether itâ€™s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, you lost your last life.)\n",
    "-**Info (dict)**: Can be ignored, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numeric-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "env.render()  #view state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-seating",
   "metadata": {},
   "source": [
    "The function (env.P) below can be used to see the relevant states and rewards for each action taken in that particular state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fancy-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 400, -1, False)],\n",
       " 1: [(1.0, 200, -1, False)],\n",
       " 2: [(1.0, 300, -1, False)],\n",
       " 3: [(1.0, 300, -1, False)],\n",
       " 4: [(1.0, 300, -10, False)],\n",
       " 5: [(1.0, 300, -10, False)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.P[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-anxiety",
   "metadata": {},
   "source": [
    "## 2. Policy Iteration\n",
    "\n",
    "**Policy Iteration** $\\rightarrow$ The algorithm redefines the policy at each step and improve the policy, and then compute the value according to this new policy until the policy converges.\n",
    "\n",
    "\n",
    "### 3.1 Functions for policy evaluation, policy iteration and value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intense-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.env.nS)\n",
    "    while True:\n",
    "        # TODO: Implement!\n",
    "        delta = 0  #delta = change in value of state from one iteration to next\n",
    "       \n",
    "        for state in range(env.env.nS):  #for all states\n",
    "            val = 0  #initiate value as 0\n",
    "            \n",
    "            for action,act_prob in enumerate(policy[state]): #for all actions/action probabilities\n",
    "                for prob,next_state,reward,done in env.env.P[state][action]:  #transition probabilities,state,rewards of each action\n",
    "                    val += act_prob * prob * (reward + discount_factor * V[next_state])  #eqn to calculate\n",
    "            delta = max(delta, np.abs(val-V[state]))\n",
    "            V[state] = val\n",
    "        if delta < theta:  #break if the change in value is less than the threshold (theta)\n",
    "            break\n",
    "    return np.array(V)\n",
    "\n",
    "def policy_iteration(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.env.nA)\n",
    "        for a in range(env.env.nA):\n",
    "            for prob, next_state, reward, done in env.env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "\n",
    "    while True:\n",
    "        # Implement this!\n",
    "        curr_pol_val = policy_eval_fn(policy, env, discount_factor)  #eval current policy\n",
    "        policy_stable = True  #Check if policy did improve (Set it as True first)\n",
    "        for state in range(env.env.nS):  #for each states\n",
    "            chosen_act = np.argmax(policy[state])  #best action (Highest prob) under current policy\n",
    "            act_values = one_step_lookahead(state,curr_pol_val)  #use one step lookahead to find action values\n",
    "            best_act = np.argmax(act_values) #find best action\n",
    "            if chosen_act != best_act:\n",
    "                policy_stable = False  #Greedily find best action\n",
    "            policy[state] = np.eye(env.env.nA)[best_act]  #update \n",
    "        if policy_stable:\n",
    "            return policy, curr_pol_val\n",
    "    \n",
    "    return policy, np.zeros(env.env.nS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-jones",
   "metadata": {},
   "source": [
    "Let's evaluate the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tired-massage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-66.26438752, -71.80792647, -71.42078771, -71.82385699,\n",
       "       -79.85874125, -79.76644406, -79.85914609, -79.85320521,\n",
       "       -78.94845049, -78.98965515, -78.18067178, -78.9907127 ,\n",
       "       -79.89007096, -79.88569714, -79.8903011 , -79.81473063,\n",
       "       -58.97388718, -71.28332886, -70.42368566, -71.3187026 ,\n",
       "       -71.76849805, -75.09061994, -74.85861632, -75.10016676,\n",
       "       -79.84505188, -79.74379722, -79.84549601, -79.83897851,\n",
       "       -79.0763058 , -79.11250001, -78.40188794, -79.11342896,\n",
       "       -79.87942213, -79.87462379, -79.87967461, -79.79676993,\n",
       "       -67.0351757 , -74.29222689, -73.47645597, -74.33919453,\n",
       "       -79.64432128, -79.7878237 , -79.77780243, -79.78823608,\n",
       "       -77.71728288, -76.22431024, -77.72383134, -77.6277274 ,\n",
       "       -79.84088218, -79.84711324, -79.72477359, -79.84727316,\n",
       "       -79.40240042, -79.37859483, -79.40365295, -78.99236538,\n",
       "       -76.72709833, -73.48229756, -76.82709925, -76.20673132,\n",
       "       -79.71298147, -79.82877523, -79.82068901, -79.82910799,\n",
       "       -76.38748358, -74.02473134, -76.39784704, -76.24575444,\n",
       "       -79.87158884, -79.87661672, -79.77789955, -79.87674576,\n",
       "       -79.30102937, -79.27318456, -79.30249441, -78.82142263,\n",
       "       -75.55219553, -70.15967578, -75.62735378, -74.86496163,\n",
       "       -79.76168806, -79.85782522, -79.85111173, -79.85810149,\n",
       "       -72.65576951, -67.85222977, -72.67683868, -72.36762995,\n",
       "       -79.89337109, -79.89754543, -79.81558612, -79.89765256,\n",
       "       -79.2409748 , -79.21073709, -79.24256573, -78.72015248,\n",
       "       -72.19821246, -60.89899061, -72.24777279, -71.52043002,\n",
       "       -73.18695081, -75.93659571, -75.74457166, -75.9444974 ,\n",
       "       -79.82788064, -79.71539385, -79.82837404, -79.82113351,\n",
       "       -78.48858122, -78.54780851, -77.38498458, -78.54932861,\n",
       "       -79.86606356, -79.8607329 , -79.86634405, -79.77424271,\n",
       "       -68.56126094, -74.47092496, -72.8234386 , -74.52531866,\n",
       "       -74.67322901, -76.82302528, -76.67289231, -76.82920318,\n",
       "       -79.78247811, -79.64029037, -79.78310178, -79.77394935,\n",
       "       -78.91251199, -78.95512562, -78.11848039, -78.95621933,\n",
       "       -79.83074281, -79.82400454, -79.83109735, -79.71467745,\n",
       "       -71.00234836, -75.49870958, -74.46919892, -75.57210301,\n",
       "       -79.46338748, -79.67991558, -79.66479447, -79.68053782,\n",
       "       -78.32626882, -77.23161036, -78.3310702 , -78.26060616,\n",
       "       -79.75997413, -79.7693762 , -79.58477979, -79.76961751,\n",
       "       -79.31510154, -79.28781746, -79.31653708, -78.8451529 ,\n",
       "       -76.86849389, -74.74673805, -77.02491668, -76.35066721,\n",
       "       -79.6423343 , -79.78664283, -79.77656523, -79.78705753,\n",
       "       -77.64863869, -76.11076178, -77.65538411, -77.55638961,\n",
       "       -79.8399993 , -79.84626539, -79.72323843, -79.84642621,\n",
       "       -79.03902152, -79.0007372 , -79.04103582, -78.37960347,\n",
       "       -76.32673822, -72.99030053, -76.42638589, -75.24616466,\n",
       "       -79.73517423, -79.84201427, -79.83455331, -79.84232129,\n",
       "       -76.53609928, -74.27054666, -76.5460364 , -76.40020063,\n",
       "       -79.88151732, -79.88615642, -79.79507255, -79.88627549,\n",
       "       -78.94126345, -78.89908398, -78.94348269, -78.21475494,\n",
       "       -75.39665938, -70.23214104, -75.46476355, -74.12464886,\n",
       "       -76.47178875, -77.8956975 , -77.79625772, -77.89978941,\n",
       "       -79.78811504, -79.649617  , -79.78872253, -79.77980762,\n",
       "       -77.12753384, -77.24010088, -75.03004953, -77.24298997,\n",
       "       -79.83512731, -79.82856391, -79.83547265, -79.72207382,\n",
       "       -72.09535248, -74.88475147, -71.3111872 , -74.95634234,\n",
       "       -77.38213209, -78.43863743, -78.36485562, -78.44167352,\n",
       "       -79.6058454 , -79.3481219 , -79.60697583, -79.59038622,\n",
       "       -78.82926446, -78.8751409 , -77.97443696, -78.87631834,\n",
       "       -79.69332791, -79.68111415, -79.69397054, -79.48295226,\n",
       "       -74.56927418, -76.31154627, -75.36116281, -76.45354205,\n",
       "       -78.9340869 , -79.3642379 , -79.3341982 , -79.36547403,\n",
       "       -79.08437437, -78.48556663, -79.08700086, -79.04845527,\n",
       "       -79.52328016, -79.54195839, -79.17524159, -79.54243778,\n",
       "       -79.2876356 , -79.25925716, -79.28912872, -78.7988375 ,\n",
       "       -76.56278091, -76.10872363, -76.88179731, -76.44671716,\n",
       "       -79.54487492, -79.72851857, -79.71569395, -79.7290463 ,\n",
       "       -78.60219512, -77.68800521, -78.60620494, -78.54735784,\n",
       "       -79.79641855, -79.8043927 , -79.64783128, -79.80459736,\n",
       "       -78.29525236, -78.22733356, -78.29882583, -77.12540647,\n",
       "       -76.32965135, -74.60908253, -76.46002447, -74.14319435,\n",
       "       -79.71789861, -79.83171226, -79.82376428, -79.83203933,\n",
       "       -78.21005266, -77.03937356, -78.21518748, -78.13982989,\n",
       "       -79.87379368, -79.8787356 , -79.78170648, -79.87886244,\n",
       "       -78.20948304, -78.13814675, -78.21323632, -76.98077382,\n",
       "       -76.21136509, -73.722569  , -76.28797616, -73.7520019 ,\n",
       "       -77.73215357, -78.64738538, -78.58346966, -78.65001549,\n",
       "       -79.86375328, -79.77473277, -79.86414375, -79.85841378,\n",
       "       -73.15770263, -73.42584765, -68.16130993, -73.43272971,\n",
       "       -79.89397076, -79.88975222, -79.89419273, -79.82130513,\n",
       "       -70.65935562, -72.25647944, -63.00516226, -72.29747018,\n",
       "       -78.62275885, -79.17855774, -79.13974331, -79.18015495,\n",
       "       -79.64397575, -79.4111886 , -79.6449968 , -79.63001236,\n",
       "       -79.38405638, -79.40819061, -78.93435498, -79.40881003,\n",
       "       -79.72299376, -79.71196178, -79.72357421, -79.53297329,\n",
       "       -76.90167619, -77.58925281, -77.31759907, -77.71909888,\n",
       "       -79.00937758, -79.40914192, -79.38122431, -79.41029073,\n",
       "       -79.5140721 , -79.19631556, -79.51546585, -79.49501183,\n",
       "       -79.55694919, -79.57430793, -79.23349674, -79.57475346,\n",
       "       -79.6219324 , -79.60687354, -79.62272472, -79.36255255,\n",
       "       -77.39829209, -77.53872901, -77.69641516, -77.71740116,\n",
       "       -79.74148344, -79.84577784, -79.83849466, -79.84607755,\n",
       "       -79.02432945, -78.38624177, -79.02712823, -78.98605413,\n",
       "       -79.88433968, -79.88886825, -79.79995459, -79.88898448,\n",
       "       -76.10655435, -75.95143011, -76.11471601, -73.43467085,\n",
       "       -75.05868769, -73.91236589, -75.12607545, -69.27839998,\n",
       "       -79.78458873, -79.87148743, -79.86541908, -79.87173715,\n",
       "       -78.92664306, -78.22465992, -78.9297221 , -78.88453497,\n",
       "       -79.90361746, -79.90739065, -79.83330737, -79.90748749,\n",
       "       -76.82653269, -76.70009474, -76.83318505, -74.6487457 ,\n",
       "       -75.71139695, -74.34416017, -75.76694583, -71.01513379,\n",
       "       -78.27640159, -78.97197743, -78.92340165, -78.97397632,\n",
       "       -79.8964148 , -79.82875959, -79.89671155, -79.8923569 ,\n",
       "       -67.02718796, -67.53558847, -57.55407827, -67.54863676,\n",
       "       -79.91937998, -79.91617399, -79.91954868, -79.86415457,\n",
       "       -66.27372105, -67.1829247 , -49.33238573, -67.20625974,\n",
       "       -79.04188594, -79.42852873, -79.4015275 , -79.42963983,\n",
       "       -79.69961969, -79.5032262 , -79.70048111, -79.68783937,\n",
       "       -79.57148454, -79.58827349, -79.25864886, -79.58870439,\n",
       "       -79.76628411, -79.7569769 , -79.76677382, -79.60597128,\n",
       "       -77.75907781, -78.15622813, -78.04817878, -78.26610241,\n",
       "       -79.15848773, -79.4980725 , -79.47435761, -79.49904838,\n",
       "       -79.660445  , -79.43842579, -79.66141882, -79.64712751,\n",
       "       -79.62362932, -79.63837488, -79.3488687 , -79.63875334,\n",
       "       -79.7358079 , -79.72528622, -79.7363615 , -79.55457713,\n",
       "       -77.90885311, -78.14099305, -78.16242714, -78.26559265,\n",
       "       -79.81338361, -79.8886599 , -79.88340322, -79.88887622,\n",
       "       -79.23607674, -78.73648507, -79.23826805, -79.20610909,\n",
       "       -79.91649274, -79.91976126, -79.85558646, -79.91984515,\n",
       "       -71.96840065, -71.64839797, -71.98523712, -66.45663001,\n",
       "       -71.57462981, -70.86147358, -71.61215143, -59.29113267,\n",
       "       -79.82638585, -79.89641584, -79.89152554, -79.89661709,\n",
       "       -79.20661795, -78.68775621, -79.20889379, -79.17549438,\n",
       "       -79.92230893, -79.92534964, -79.86564746, -79.92542768,\n",
       "       -75.16143796, -74.96865681, -75.17158087, -71.84094727,\n",
       "       -74.50986535, -73.61151385, -74.5500549 , -67.1776962 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "random_policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
    "policy_eval(random_policy,env,discount_factor=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-magnet",
   "metadata": {},
   "source": [
    "Now let's use policy iteration to improve our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brilliant-christian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "pol_iter_policy = policy_iteration(env,policy_eval,discount_factor=0.99)\n",
    "pol_iter_policy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-statistics",
   "metadata": {},
   "source": [
    "## Let's watch how our optimal policies works in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "negative-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_policy(policy):\n",
    "    curr_state = env.reset()\n",
    "    counter = 0\n",
    "    reward = None\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(np.argmax(policy[0][curr_state])) \n",
    "        curr_state = state\n",
    "        counter += 1\n",
    "        env.env.s = curr_state\n",
    "        env.render()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "guided-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| :\u001b[42m_\u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "view_policy(pol_iter_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-benchmark",
   "metadata": {},
   "source": [
    "## View optimal policy in action (animated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "intermediate-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "def view_policy_anim(policy):\n",
    "    penalties, reward = 0, 0\n",
    "\n",
    "    frames = [] # for animation\n",
    "\n",
    "    done = False\n",
    "    curr_state = env.reset()\n",
    "    while not done:\n",
    "        action = np.argmax(policy[0][curr_state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        curr_state = state\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "    def print_frames(frames):\n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame']) #.getvalue())\n",
    "            print(\"Timestep: {}\".format(i+1))\n",
    "            print(\"State: {}\".format(frame['state']))\n",
    "            print(\"Action: {}\".format(frame['action']))\n",
    "            print(\"Reward: {}\".format(frame['reward']))\n",
    "            sleep(.2)\n",
    "\n",
    "    print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aggressive-henry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 12\n",
      "State: 475\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "view_policy_anim(pol_iter_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-glass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
